{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36817858940fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:36.674524500Z",
     "start_time": "2023-11-30T09:51:36.663524700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy\n",
    "from solution import ppo_loss, value_loss\n",
    "import torch\n",
    "from utils.plotting import plot_training\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE = True\n",
    "N=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd343db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(env, pi, T):\n",
    "    \"\"\"given an environment env, a stochastic policy pi and number of timesteps T, interact with the environment for T steps \n",
    "    using actions sampled from policy. Return torch tensors of collected states, actions and rewards\"\"\"\n",
    "    states = np.zeros((T + 1, N, env.num_states), dtype=float)  # states from s(0) to s(T+1)\n",
    "    actions = np.zeros((T, N, env.num_actions), dtype=float)  # actions from a(0) to a(T)\n",
    "    rewards = np.zeros((T, N), dtype=float)  # rewards from r(0) to r(T)\n",
    "\n",
    "    s = env.vector_reset()\n",
    "    states[0] = s\n",
    "    for t in range(T):\n",
    "        a = pi.sample_actions(torch.tensor(states[t]).float())  # policy needs float torch tensor (N, state_dim)\n",
    "        s_next, r = env.vector_step(np.array(a))  # env needs numpy array of (Nx1)\n",
    "        states[t + 1], actions[t], rewards[t] = s_next, a, r\n",
    "\n",
    "    tensor_s = torch.tensor(states).float()  # (T+1, N, state_dim)  care for the extra timestep at the end!\n",
    "    tensor_a = torch.tensor(actions).float()  # (T, N, 1)\n",
    "    tensor_r = torch.tensor(rewards).float()  # (T, N)\n",
    "\n",
    "    return tensor_s, tensor_a, tensor_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030dc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution import discount_cum_sum\n",
    "\n",
    "def compute_advantage_estimates(tensor_r, values, gamma, bootstrap=False):\n",
    "    \"\"\"given reward tensor (T, N), value estimates tensor (T+1, N) and gamma scalar\"\"\"\n",
    "    if bootstrap:  # use last value estimates as a return estimate\n",
    "        terminal_value_estimates = values[-1].unsqueeze(0)  # values of the last states (1, N)\n",
    "        rs_v = torch.cat((tensor_r, terminal_value_estimates), dim=0)\n",
    "        value_targets = discount_cum_sum(rs_v, gamma)[:-1]\n",
    "    else:\n",
    "        value_targets = discount_cum_sum(tensor_r, gamma)\n",
    "    advantages = value_targets - values[:-1]\n",
    "    return value_targets, advantages\n",
    "\n",
    "\n",
    "def compute_gae(tensor_r, values, gamma, lambda_):\n",
    "    \"\"\"generalized advantage estimation (GAE) implementation\"\"\"\n",
    "    delta_t = tensor_r + gamma * values[1:] - values[:-1]\n",
    "    advantages = discount_cum_sum(delta_t, gamma * lambda_)\n",
    "    value_targets = advantages + values[:-1]\n",
    "    return value_targets, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(pi, config, T=128, deterministic=True):\n",
    "    test_env = WalkerEnv(config)\n",
    "    mean_reward = 0\n",
    "    \n",
    "    s = test_env.vector_reset()\n",
    "    x = s[0, 0]\n",
    "    for i in range(T):\n",
    "        with torch.no_grad():\n",
    "            if deterministic:\n",
    "                actions = pi.determine_actions(torch.tensor(s).float()) \n",
    "            else:\n",
    "                actions = pi.sample_actions(torch.tensor(s).float()) \n",
    "        s, r = test_env.vector_step(actions.numpy())\n",
    "        x = max(x, s[0, 0])\n",
    "        mean_reward += sum(r) / (T * config['N'])\n",
    "\n",
    "    print(f\"Max x: {x}\")\n",
    "    test_env.close()\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walker_reward(state, action):\n",
    "    pos = state[:15]  # first 15 elements of state vector are generalized coordinates [xyz, quat, joint_angles]\n",
    "    vel = state[15:]  # last 14 elements of state vector are generalized velocities [xyz_vel, omega, joint_velocities]\n",
    "    return vel[0]*1.5  # return the x velocity as the reward by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b67909",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "base_config = {\n",
    "    \"N\": N,\n",
    "    \"vis\": False,\n",
    "    \"track\": 0,\n",
    "    # \"reward_fcn\": walker_reward\n",
    "}\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# training parameters\n",
    "epochs = 500\n",
    "gamma = 0.95\n",
    "epsilon = 0.2\n",
    "sgd_iters = 5\n",
    "T = 512\n",
    "# policy, environment and optimizer\n",
    "pi = WalkerPolicy(state_dim=29, action_dim=8)\n",
    "train_env = WalkerEnv(base_config)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(pi.parameters(), lr=lr)\n",
    "mean_rewards, p_losses, v_losses = np.zeros(epochs), np.zeros(epochs), np.zeros(epochs)  # for logging mean rewards over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    tensor_s, tensor_a, tensor_r = sample_trajectories(train_env, pi, T)  # collect trajectories using current policy\n",
    "\n",
    "    with torch.no_grad():  # compute the old probabilities\n",
    "        logp_old = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "\n",
    "    for i in range(sgd_iters):  # we can even do multiple gradient steps\n",
    "        values = pi.value_estimates(tensor_s)  # estimate value function for all states\n",
    "        logp = pi.log_prob(tensor_a, tensor_s[:T]).squeeze(2)  # compute log(pi(a_t | s_t))\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            value_targets, advantage_estimates = compute_advantage_estimates(tensor_r, values, gamma, bootstrap=True)\n",
    "            advantage_estimates = (advantage_estimates - advantage_estimates.mean()) / advantage_estimates.std()  # normalize advantages\n",
    "\n",
    "        L_v = value_loss(values[:T], value_targets)  # add the value loss\n",
    "\n",
    "        p_ratios = torch.exp(logp - logp_old)  # compute the ratios r_\\theta(a_t | s_t)\n",
    "        L_ppo = ppo_loss(p_ratios, advantage_estimates, epsilon=epsilon)  # compute the policy gradient loss\n",
    "        total_loss = L_v + L_ppo\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()  # backprop and gradient step\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch %d, mean reward: %.3f, value loss: %.3f' % (epoch, tensor_r.mean(), L_v.item()))\n",
    "    mean_rewards[epoch] = tensor_r.mean()\n",
    "    v_losses[epoch] = L_v.item()\n",
    "    p_losses[epoch] = L_ppo.item()\n",
    "\n",
    "plot_training(mean_rewards, p_losses, v_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"N\": 4,\n",
    "    \"vis\": True,\n",
    "    \"track\": 0,\n",
    "    \"reward_fcn\": walker_reward\n",
    "}\n",
    "test_policy(pi, config, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.save_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f11236",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.save_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
